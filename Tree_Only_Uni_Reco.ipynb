{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz+j1kVQbcl4kII+X4Y1a0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angatang/University-Recommender/blob/Tree-Only/Tree_Only_Uni_Reco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "wu84csNhePWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "CS58AMTpc1MM",
        "outputId": "c1aa43f8-987d-4bde-d26c-4bea54ab8d3a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1d44f875bd3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Data_21-24 (1)-1.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "df = pd.read_excel(\"/content/Data_21-24 (1)-1.xlsx\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Is Competitive Exam Given' in df.columns:\n",
        "    print(\"Column 'Is Competitive Exam Given' exists in df.\")\n",
        "    print(\"Value Counts (including NaNs):\")\n",
        "    print(df['Is Competitive Exam Given'].value_counts(dropna=False))\n",
        "    # Check data type as well\n",
        "    print(f\"Data type: {df['Is Competitive Exam Given'].dtype}\")\n",
        "\n",
        "    # Also, let's re-check the scores *before* they are potentially overwritten\n",
        "    # Make sure the column name is correct for your raw data before conversion\n",
        "    print(\"\\nChecking original competitive scores in df_us (after potential to_numeric):\")\n",
        "    original_scores = pd.to_numeric(df['Total Score (Competitive Exam)'], errors='coerce')\n",
        "    print(original_scores.value_counts(dropna=False))\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Column 'Is Competitive Exam Given' DOES NOT EXIST in df!\")"
      ],
      "metadata": {
        "id": "4RNdVWPec2Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nParsing College Details...\")\n",
        "\n",
        "def parse_college_details(details):\n",
        "    if pd.isna(details):\n",
        "        return pd.Series([np.nan] * 4) # Return NaNs if input is NaN\n",
        "\n",
        "    # Regex to capture: Major, College, University, Year\n",
        "    # Adjust pattern if format varies significantly\n",
        "    # Using non-greedy matching (.*?) and being specific about keywords\n",
        "    pattern = r\"^(.*?),\\s*College:\\s*(.*?),\\s*University:\\s*(.*?),\\s*Year:\\s*(\\d{4})$\"\n",
        "    match = re.search(pattern, str(details).strip())\n",
        "\n",
        "    if match:\n",
        "        ug_major = match.group(1).strip()\n",
        "        ug_college = match.group(2).strip()\n",
        "        ug_university = match.group(3).strip()\n",
        "        ug_year = int(match.group(4)) # Convert year to integer\n",
        "        return pd.Series([ug_major, ug_college, ug_university, ug_year])\n",
        "    else:\n",
        "        # Attempt simpler parsing if primary fails (e.g. if missing parts)\n",
        "        # Fallback: return NaNs or try extracting at least year/major if possible\n",
        "        # This part might need refinement based on actual variations in your data\n",
        "        year_match = re.search(r\"Year:\\s*(\\d{4})\", str(details))\n",
        "        ug_year = int(year_match.group(1)) if year_match else np.nan\n",
        "        # Add more fallback logic if needed\n",
        "        return pd.Series([np.nan] * 3 + [ug_year]) # Example fallback\n",
        "\n",
        "# Apply the function to create new columns\n",
        "parsed_cols = ['UG_Major', 'UG_College_Name', 'UG_University_Name', 'UG_Year_of_Graduation']\n",
        "df[parsed_cols] = df['College Details'].apply(parse_college_details)\n"
      ],
      "metadata": {
        "id": "umqQ-CWrc67U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCalculating Overall GPA on 4.0 Scale...\")\n",
        "\n",
        "# Define the list of score columns\n",
        "score_columns = [\n",
        "    'First Year Score', 'Second Year Score', 'Third Year Score',\n",
        "    'Fourth Year Score', 'Fifth Year Score', 'Sixth Year Score',\n",
        "    'Seventh Year Score', 'Eighth Year Score'\n",
        "]\n",
        "\n",
        "# Convert score columns to numeric, coercing errors to NaN\n",
        "for col in score_columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "def calculate_gpa_4_scale(row):\n",
        "    system = row['Grading Systen']\n",
        "    scores = row[score_columns].tolist()\n",
        "    valid_scores = []\n",
        "\n",
        "    if pd.isna(system):\n",
        "        return np.nan\n",
        "\n",
        "    # Determine scale and outlier bounds\n",
        "    max_scale = np.nan\n",
        "    lower_bound = np.nan\n",
        "    upper_bound = np.nan\n",
        "\n",
        "    if system == 'CGPA10':\n",
        "        max_scale = 10.0\n",
        "        lower_bound = 1.0 # Adjust if lower scores are valid\n",
        "        upper_bound = 10.0\n",
        "    elif system == 'GPA7':\n",
        "        max_scale = 7.0\n",
        "        lower_bound = 1.0 # Adjust if lower scores are valid\n",
        "        upper_bound = 7.0\n",
        "    elif system == 'Default': # Percentage scale\n",
        "        max_scale = 100.0\n",
        "        lower_bound = 35.0 # Assuming scores below 35% are outliers/fails to ignore\n",
        "        upper_bound = 100.0\n",
        "    else: # Handle unexpected systems\n",
        "        return np.nan\n",
        "\n",
        "    # Filter scores\n",
        "    for score in scores:\n",
        "        if pd.notna(score) and lower_bound <= score <= upper_bound:\n",
        "             valid_scores.append(score)\n",
        "\n",
        "    # Calculate average and convert\n",
        "    if not valid_scores: # Check if list is empty\n",
        "        return np.nan\n",
        "    else:\n",
        "        average_score = sum(valid_scores) / len(valid_scores)\n",
        "        return (average_score / max_scale) * 4.0\n",
        "\n",
        "# Apply the function row-wise\n",
        "df['Overall_GPA_4_Scale'] = df.apply(calculate_gpa_4_scale, axis=1)\n"
      ],
      "metadata": {
        "id": "D7Lm0x8Ac7no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'Is Competitive Exam Given' in df.columns:\n",
        "    print(\"Column 'Is Competitive Exam Given' exists in df.\")\n",
        "    print(\"Value Counts (including NaNs):\")\n",
        "    print(df['Is Competitive Exam Given'].value_counts(dropna=False))\n",
        "    # Check data type as well\n",
        "    print(f\"Data type: {df['Is Competitive Exam Given'].dtype}\")\n",
        "\n",
        "    # Also, let's re-check the scores *before* they are potentially overwritten\n",
        "    # Make sure the column name is correct for your raw data before conversion\n",
        "    print(\"\\nChecking original competitive scores in df_us (after potential to_numeric):\")\n",
        "    original_scores = pd.to_numeric(df['Total Score (Competitive Exam)'], errors='coerce')\n",
        "    print(original_scores.value_counts(dropna=False))\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Column 'Is Competitive Exam Given' DOES NOT EXIST in df!\")"
      ],
      "metadata": {
        "id": "Qg2NfGADc-hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nHandling Test Scores\")\n",
        "\n",
        "# Define Max Scores\n",
        "MAX_SCORES = {\n",
        "    'GRE': 340,\n",
        "    'GMAT': 800,  # Traditional GMAT Max\n",
        "    'SAT': 1600,\n",
        "    'ACT': 36\n",
        "}\n",
        "\n",
        "# --- Language Exam (No changes needed here from previous correction) ---\n",
        "lang_score_col = 'Total Score (Language Exam)'\n",
        "lang_given_col = 'Is Language Exam Given'\n",
        "df[lang_score_col] = pd.to_numeric(df[lang_score_col], errors='coerce')\n",
        "df[lang_given_col] = pd.to_numeric(df[lang_given_col], errors='coerce')\n",
        "df.loc[(df[lang_given_col] == 0.0) | (df[lang_given_col].isna()), lang_score_col] = 0\n",
        "df.loc[(df[lang_given_col] == 1.0) & (df[lang_score_col].isna()), lang_score_col] = -2\n",
        "\n",
        "# --- Competitive Exam ---\n",
        "comp_score_col = 'Total Score (Competitive Exam)'\n",
        "comp_given_col = 'Is Competitive Exam Given'\n",
        "comp_exam_type_col = 'Competitive Exam' # Column specifying GRE/GMAT/SAT/ACT\n",
        "\n",
        "# Ensure score column is numeric\n",
        "df[comp_score_col] = pd.to_numeric(df[comp_score_col], errors='coerce')\n",
        "# Ensure 'Is Given' column is numeric/float\n",
        "df[comp_given_col] = pd.to_numeric(df[comp_given_col], errors='coerce')\n",
        "\n",
        "# Apply logic based on the 'Is Given' flag (1.0 = Yes, 0.0 = No, NaN = Treat as No)\n",
        "df.loc[(df[comp_given_col] == 0.0) | (df[comp_given_col].isna()), comp_score_col] = 0\n",
        "df.loc[(df[comp_given_col] == 1.0) & (df[comp_score_col].isna()), comp_score_col] = -2\n",
        "\n",
        "# !!! NEW CAPPING LOGIC !!!\n",
        "# Ensure the Exam Type column is string and handle potential NaNs\n",
        "df[comp_exam_type_col] = df[comp_exam_type_col].astype(str).fillna('UNKNOWN')\n",
        "\n",
        "print(\"Applying score capping...\")\n",
        "for exam_type, max_score in MAX_SCORES.items():\n",
        "    # Condition for rows matching the exam type AND score exceeding max\n",
        "    condition = (\n",
        "        (df[comp_exam_type_col].str.upper() == exam_type) &\n",
        "        (df[comp_score_col] > max_score)\n",
        "    )\n",
        "    # Apply capping\n",
        "    count_capped = condition.sum()\n",
        "    if count_capped > 0:\n",
        "        print(f\"Capping {count_capped} scores for {exam_type} at {max_score}\")\n",
        "        df.loc[condition, comp_score_col] = max_score\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Sample processed test scores (Corrected + Capped):\")\n",
        "print(df[[lang_given_col, lang_score_col, comp_given_col, comp_score_col, comp_exam_type_col]].head())\n",
        "print(\"\\nValue counts for Competitive Score after correction + capping:\")\n",
        "# Check if capping worked by looking at max values per exam type if needed\n",
        "# print(df_us.groupby(comp_exam_type_col)[comp_score_col].max())\n",
        "print(df[comp_score_col].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "qOrs-joOc_FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index().rename(columns={'index': 'StudentID'}) # Use index as StudentID\n",
        "\n",
        "print(\"\\nColumns PRESENT in 'df' AFTER reset_index/rename:\")\n",
        "print(df.columns.tolist())\n",
        "# -----------------------------------------\n",
        "\n",
        "# --- Define Feature Columns (Adjust based on actual cleaned df) ---\n",
        "print(\"\\nDefining feature columns list...\")\n",
        "student_features_to_keep = [\n",
        "    'Overall_GPA_4_Scale',\n",
        "    'Total Score (Language Exam)', # Use cleaned/capped scores\n",
        "    'Total Score (Competitive Exam)', # Use cleaned/capped scores\n",
        "    'Language Exam',\n",
        "    'Competitive Exam',\n",
        "    'Future Desired Degree',\n",
        "    'Desired Field of Study',\n",
        "    'Desired Specialization'\n",
        "]\n",
        "required_cols_check = student_features_to_keep + ['Suggested Universities', 'StudentID']\n",
        "\n",
        "# --- Check for required columns (This is where the error occurs) ---\n",
        "print(\"\\nChecking if required columns exist...\")\n",
        "missing_cols_check = [col for col in required_cols_check if col not in df.columns]\n",
        "\n",
        "if missing_cols_check:\n",
        "    print(f\"DEBUG: Columns currently in df before raising error: {df.columns.tolist()}\") # Add another check just before error\n",
        "    raise ValueError(f\"Missing required columns after initial cleaning: {missing_cols_check}\")\n",
        "else:\n",
        "    print(\"All required columns found.\")\n",
        "\n",
        "\n",
        "# --- This part will only run if the check passes ---\n",
        "df_for_restructure = df[required_cols_check].copy() # Keep only needed columns\n",
        "print(\"Successfully created df_for_restructure.\")"
      ],
      "metadata": {
        "id": "HBT7Jo2PdBCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parsing Function ---\n",
        "def parse_suggestions(text):\n",
        "    suggestions = []\n",
        "    if isinstance(text, str):\n",
        "        lines = text.split('\\n')\n",
        "        for line in lines:\n",
        "            parts = line.split('|')\n",
        "            if len(parts) >= 2:\n",
        "                uni_name = parts[0].strip()\n",
        "                status = parts[1].strip().upper()\n",
        "                if uni_name:\n",
        "                    suggestions.append({'UniversityName': uni_name, 'Status': status})\n",
        "    return suggestions\n"
      ],
      "metadata": {
        "id": "9JYBhcFAdEfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generate Positive and Potential Negative Examples ---\n",
        "print(\"Parsing suggestions and generating examples...\")\n",
        "all_examples_rows = []\n",
        "positive_statuses = ['ADMITRECEIVED', 'FINALIZED', 'ACCEPTED'] # Define positive statuses\n",
        "\n",
        "for index, student_row in df_for_restructure.iterrows():\n",
        "    student_id = student_row['StudentID']\n",
        "    suggestions_text = student_row['Suggested Universities']\n",
        "    parsed_list = parse_suggestions(suggestions_text)\n",
        "    if not parsed_list: continue\n",
        "\n",
        "    student_features = student_row[student_features_to_keep].to_dict()\n",
        "\n",
        "    for suggestion in parsed_list:\n",
        "        example_row = student_features.copy()\n",
        "        example_row['StudentID'] = student_id\n",
        "        example_row['UniversityName'] = suggestion['UniversityName']\n",
        "        status = suggestion['Status']\n",
        "        example_row['Admitted'] = 1 if status in positive_statuses else 0\n",
        "        all_examples_rows.append(example_row)\n",
        "\n",
        "if not all_examples_rows:\n",
        "     raise ValueError(\"No examples generated. Check 'Suggested Universities' data/parsing.\")\n",
        "\n",
        "all_examples_df = pd.DataFrame(all_examples_rows)\n",
        "print(f\"Generated {all_examples_df.shape[0]} raw examples.\")\n"
      ],
      "metadata": {
        "id": "mXNyDND7dGxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ensure Consistency (Prioritize Positive) ---\n",
        "print(\"Resolving conflicts...\")\n",
        "all_examples_df['Student_Uni_Key'] = all_examples_df['StudentID'].astype(str) + \"||\" + all_examples_df['UniversityName']\n",
        "positive_df = all_examples_df[all_examples_df['Admitted'] == 1].drop_duplicates(subset=['Student_Uni_Key'])\n",
        "negative_df = all_examples_df[all_examples_df['Admitted'] == 0]\n",
        "negative_df = negative_df[~negative_df['Student_Uni_Key'].isin(set(positive_df['Student_Uni_Key']))].drop_duplicates(subset=['Student_Uni_Key'])\n",
        "interaction_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
        "interaction_df = interaction_df.drop(columns=['Student_Uni_Key'])\n",
        "print(f\"Interaction DataFrame shape after de-duplication: {interaction_df.shape}\")\n",
        "print(\"Final 'Admitted' distribution:\")\n",
        "print(interaction_df['Admitted'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "0C-Qn20bdJtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Feature Selection & NaN Handling ---\n",
        "print(\"Selecting features and handling NaNs...\")\n",
        "# Define feature types (UniversityName is now categorical)\n",
        "numerical_features = ['Overall_GPA_4_Scale', 'Total Score (Language Exam)', 'Total Score (Competitive Exam)']\n",
        "categorical_features = [\n",
        "    'Language Exam', 'Competitive Exam', 'Future Desired Degree',\n",
        "    'Desired Field of Study', 'Desired Specialization', 'UniversityName'\n",
        "]\n",
        "features_to_use = numerical_features + categorical_features\n",
        "# Ensure columns exist\n",
        "available_features = [col for col in features_to_use if col in interaction_df.columns]\n",
        "interaction_df_final = interaction_df[available_features + ['Admitted']].copy()\n",
        "\n",
        "# Fill NaNs\n",
        "for col in numerical_features:\n",
        "    if col in interaction_df_final.columns:\n",
        "        interaction_df_final[col] = interaction_df_final[col].fillna(interaction_df_final[col].median()) # Use median for scores\n",
        "for col in categorical_features:\n",
        "     if col in interaction_df_final.columns:\n",
        "        interaction_df_final[col] = interaction_df_final[col].fillna('Unknown').astype(str)\n",
        "\n",
        "X = interaction_df_final.drop('Admitted', axis=1)\n",
        "y = interaction_df_final['Admitted']\n"
      ],
      "metadata": {
        "id": "pQE-M9BIdLPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train-Test Split ---\n",
        "print(\"Splitting data into Training (80%) and Testing (20%) sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20, # Held-out test set\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "print(f\"Training set shape: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
        "print(f\"Testing set shape: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
        "\n",
        "\n",
        "# --- Data Restructuring Complete ---\n",
        "print(\"\\nData restructured. Ready for preprocessing pipeline and hyperparameter tuning.\")\n"
      ],
      "metadata": {
        "id": "8KxG4Y4mdNJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 2: Defining Preprocessor & Tuning Models\n",
        "\n",
        "# --- 1. Define Preprocessing Pipeline ---\n",
        "# Re-identify numerical and categorical columns IN X_train (in case some were dropped)\n",
        "current_numerical = [col for col in numerical_features if col in X_train.columns]\n",
        "current_categorical = [col for col in categorical_features if col in X_train.columns]\n",
        "\n",
        "# WARNING: OneHotEncoding 'UniversityName' might create VERY many features.\n",
        "# Consider HashingVectorizer(n_features=...) or Target Encoding for UniversityName\n",
        "# if dimensionality becomes problematic. For now, using OHE.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), current_numerical),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), current_categorical)\n",
        "    ],\n",
        "    remainder='drop' # Drop any columns not specified (like StudentID if it remained)\n",
        ")"
      ],
      "metadata": {
        "id": "43ZL_szXdPGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Define Parameter Grids ---\n",
        "# Reduced grids for faster demonstration - expand ranges for thorough search\n",
        "param_grid_rf = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [10, 20, None],\n",
        "    'classifier__min_samples_split': [2, 5],\n",
        "    'classifier__class_weight': ['balanced', None] # Handle imbalance\n",
        "}\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'classifier__n_estimators': [1200],\n",
        "    'classifier__max_depth': [75],\n",
        "    'classifier__learning_rate': [0.4],\n",
        "    'classifier__gamma': [0]\n",
        "    # Add 'scale_pos_weight' if severe imbalance needs addressing\n",
        "}"
      ],
      "metadata": {
        "id": "ildspF0FdUHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Set up Pipelines and GridSearchCV ---\n",
        "# Random Forest Pipeline\n",
        "pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))])\n",
        "\n",
        "# XGBoost Pipeline\n",
        "pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                             ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))])\n",
        "\n",
        "# GridSearchCV Instances\n",
        "search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n"
      ],
      "metadata": {
        "id": "iUzQ0hArdWZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Run Hyperparameter Tuning (on Training Data) ---\n",
        "print(\"\\nStarting GridSearchCV for Random Forest...\")\n",
        "search_rf.fit(X_train, y_train)\n",
        "print(f\"Best RF Params: {search_rf.best_params_}\")\n",
        "print(f\"Best RF CV ROC-AUC: {search_rf.best_score_:.4f}\")\n",
        "best_rf_model = search_rf.best_estimator_ # Save the best pipeline\n",
        "\n",
        "print(\"\\nStarting GridSearchCV for XGBoost...\")\n",
        "search_xgb.fit(X_train, y_train)\n",
        "print(f\"Best XGB Params: {search_xgb.best_params_}\")\n",
        "print(f\"Best XGB CV ROC-AUC: {search_xgb.best_score_:.4f}\")\n",
        "best_xgb_model = search_xgb.best_estimator_ # Save the best pipeline\n"
      ],
      "metadata": {
        "id": "XsK89LHBdYQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Evaluate Best Models on Test Set ---\n",
        "print(\"\\n--- Evaluating Best Models on Test Set ---\")\n",
        "\n",
        "# Random Forest Evaluation\n",
        "print(\"\\n--- Random Forest (Test Set) ---\")\n",
        "y_pred_rf_test = best_rf_model.predict(X_test)\n",
        "y_prob_rf_test = best_rf_model.predict_proba(X_test)[:, 1]\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_prob_rf_test):.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf_test))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf_test))\n",
        "\n",
        "# XGBoost Evaluation\n",
        "print(\"\\n--- XGBoost (Test Set) ---\")\n",
        "y_pred_xgb_test = best_xgb_model.predict(X_test)\n",
        "y_prob_xgb_test = best_xgb_model.predict_proba(X_test)[:, 1]\n",
        "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_prob_xgb_test):.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb_test))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_xgb_test))\n",
        "\n",
        "# --- Tuning and Evaluation Complete ---\n",
        "print(\"\\nHyperparameter tuning and test set evaluation complete.\")"
      ],
      "metadata": {
        "id": "xkaCdp4Xda26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cv_results(search_cv_object, param_x, param_hue, model_name):\n",
        "    \"\"\"\n",
        "    Plots mean CV score against one hyperparameter, potentially hueing by another.\n",
        "    Assumes search_cv_object is a fitted GridSearchCV instance.\n",
        "    \"\"\"\n",
        "    cv_results = pd.DataFrame(search_cv_object.cv_results_)\n",
        "\n",
        "    # Extract parameter names correctly (they are prefixed)\n",
        "    param_x_col = f'param_classifier__{param_x}'\n",
        "    param_hue_col = f'param_classifier__{param_hue}' if param_hue else None\n",
        "\n",
        "    # Check if columns exist\n",
        "    required_plot_cols = [param_x_col, 'mean_test_score']\n",
        "    if param_hue_col:\n",
        "        required_plot_cols.append(param_hue_col)\n",
        "    if not all(col in cv_results.columns for col in required_plot_cols):\n",
        "        print(f\"Error: Not all required columns found in cv_results_ for plotting. Available: {cv_results.columns.tolist()}\")\n",
        "        print(f\"Looking for: {required_plot_cols}\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(\n",
        "        data=cv_results,\n",
        "        x=param_x_col,\n",
        "        y='mean_test_score', # Mean score on validation folds during CV\n",
        "        hue=param_hue_col, # Optional: Color lines by another parameter\n",
        "        marker='o',\n",
        "        errorbar=('ci', 95) # Show confidence interval if multiple runs per point\n",
        "    )\n",
        "    plt.title(f'{model_name}: Mean ROC-AUC vs. {param_x}' + (f' (Grouped by {param_hue})' if param_hue else ''))\n",
        "    plt.xlabel(param_x)\n",
        "    plt.ylabel('Mean ROC-AUC (Cross-Validation)')\n",
        "    plt.grid(True)\n",
        "    if param_hue_col:\n",
        "        plt.legend(title=param_hue, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "    else:\n",
        "        plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mbSOQB2xgTOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot for Random Forest ---\n",
        "if 'search_rf' in locals() and hasattr(search_rf, 'best_estimator_'):\n",
        "    print(\"\\n--- Visualizing Random Forest Tuning Results ---\")\n",
        "    # Example: Plot ROC-AUC vs. n_estimators, grouped by max_depth\n",
        "    plot_cv_results(search_rf, param_x='n_estimators', param_hue='max_depth', model_name='Random Forest')\n",
        "    # Example: Plot ROC-AUC vs. min_samples_split, grouped by class_weight\n",
        "    plot_cv_results(search_rf, param_x='min_samples_split', param_hue='class_weight', model_name='Random Forest')\n",
        "else:\n",
        "    print(\"Cannot plot RF results: 'search_rf' not found or not fitted.\")\n"
      ],
      "metadata": {
        "id": "a5rcEFUcgUyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot for XGBoost ---\n",
        "if 'search_xgb' in locals() and hasattr(search_xgb, 'best_estimator_'):\n",
        "    print(\"\\n--- Visualizing XGBoost Tuning Results ---\")\n",
        "    # Example: Plot ROC-AUC vs. n_estimators, grouped by max_depth\n",
        "    plot_cv_results(search_xgb, param_x='n_estimators', param_hue='max_depth', model_name='XGBoost')\n",
        "    # Example: Plot ROC-AUC vs. learning_rate, grouped by gamma\n",
        "    plot_cv_results(search_xgb, param_x='learning_rate', param_hue='gamma', model_name='XGBoost')\n",
        "else:\n",
        "    print(\"Cannot plot XGBoost results: 'search_xgb' not found or not fitted.\")\n"
      ],
      "metadata": {
        "id": "yYSaHXsWgZ0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GridSearchCV objects 'search_rf' and 'search_xgb' exist and have been fitted.\n",
        "\n",
        "print(\"\\n--- Analyzing Feature Importances ---\")\n",
        "\n",
        "# --- Function to Get and Plot Importances ---\n",
        "def plot_feature_importances(search_cv_object, X_train_cols, model_name, top_n=20):\n",
        "    \"\"\"\n",
        "    Extracts, processes, and plots feature importances from a fitted\n",
        "    GridSearchCV object containing a Pipeline(preprocessor, classifier).\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- {model_name} Feature Importances ---\")\n",
        "    try:\n",
        "        # 1. Access the best pipeline found by GridSearchCV\n",
        "        best_pipeline = search_cv_object.best_estimator_\n",
        "\n",
        "        # 2. Access the fitted classifier step\n",
        "        classifier = best_pipeline.named_steps['classifier']\n",
        "\n",
        "        # 3. Access the fitted preprocessor step\n",
        "        preprocessor_fitted = best_pipeline.named_steps['preprocessor']\n",
        "\n",
        "        # 4. Get feature importances from the classifier\n",
        "        importances = classifier.feature_importances_\n",
        "\n",
        "        # 5. Get feature names AFTER preprocessing (including OHE columns)\n",
        "        # Use the get_feature_names_out method of the fitted ColumnTransformer\n",
        "        try:\n",
        "            feature_names = preprocessor_fitted.get_feature_names_out(input_features=X_train_cols)\n",
        "        except TypeError:\n",
        "             # Older sklearn versions might not support input_features argument directly\n",
        "             # Or try without input_features if it fails, though less reliable with complex transformers\n",
        "             print(\"Warning: Using get_feature_names_out without input_features, names might be generic.\")\n",
        "             feature_names = preprocessor_fitted.get_feature_names_out()\n",
        "\n",
        "\n",
        "        if len(importances) != len(feature_names):\n",
        "            print(f\"Error: Mismatch between number of importances ({len(importances)}) and feature names ({len(feature_names)}). Cannot plot.\")\n",
        "            return None\n",
        "\n",
        "        # 6. Create a DataFrame for easier handling\n",
        "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "\n",
        "        # 7. Sort by importance and select top N\n",
        "        importance_df = importance_df.sort_values(by='Importance', ascending=False).head(top_n)\n",
        "\n",
        "        # 8. Print Top N Importances\n",
        "        print(f\"\\nTop {top_n} Features:\")\n",
        "        print(importance_df)\n",
        "\n",
        "        # 9. Plot Top N Importances\n",
        "        plt.figure(figsize=(10, top_n / 2.5)) # Adjust figure size based on N\n",
        "        sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\n",
        "        plt.title(f'Top {top_n} Feature Importances ({model_name})')\n",
        "        plt.xlabel('Importance Score')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return importance_df # Return the dataframe for potential further use\n",
        "\n",
        "    except AttributeError as e:\n",
        "        print(f\"Error accessing attributes, likely model or preprocessor steps incorrect: {e}\")\n",
        "        print(\"Make sure GridSearchCV has run and 'best_estimator_' contains a Pipeline named 'preprocessor' and 'classifier'.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during importance calculation/plotting: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Generate Importances for Random Forest ---\n",
        "# Ensure X_train still holds the original columns before preprocessing within the pipeline\n",
        "if 'search_rf' in locals() and hasattr(search_rf, 'best_estimator_'):\n",
        "    plot_feature_importances(search_rf, X_train.columns, \"Random Forest\")\n",
        "else:\n",
        "    print(\"GridSearchCV object 'search_rf' not found or not fitted. Skipping RF importance plot.\")\n",
        "\n",
        "\n",
        "# --- Generate Importances for XGBoost ---\n",
        "if 'search_xgb' in locals() and hasattr(search_xgb, 'best_estimator_'):\n",
        "    plot_feature_importances(search_xgb, X_train.columns, \"XGBoost\")\n",
        "else:\n",
        "     print(\"GridSearchCV object 'search_xgb' not found or not fitted. Skipping XGBoost importance plot.\")"
      ],
      "metadata": {
        "id": "pcI2B-00eOXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Assuming 'best_xgb_model' is the fitted Pipeline object from GridSearchCV\n",
        "# Assuming 'X_train' is the training feature DataFrame (before preprocessing in pipeline)\n",
        "# 'interaction_df_final' or similar holds the original data used for X/y split\n",
        "\n",
        "print(\"\\n--- Defining Recommendation Function using XGBoost Model ---\")\n",
        "\n",
        "def recommend_universities_xgb(student_profile_dict, model_pipeline, candidate_universities, original_feature_columns, top_n=10):\n",
        "    \"\"\"\n",
        "    Generates university recommendations for a student using a trained XGBoost pipeline.\n",
        "\n",
        "    Args:\n",
        "        student_profile_dict (dict): Dictionary containing the new student's features.\n",
        "                                     Keys must match original feature columns before preprocessing.\n",
        "        model_pipeline (Pipeline): The fitted Scikit-learn pipeline (preprocessor + XGBoost).\n",
        "        candidate_universities (list): A list of university names to score.\n",
        "        original_feature_columns (list): The list of feature column names expected by the\n",
        "                                         pipeline's preprocessor (e.g., X_train.columns).\n",
        "        top_n (int): The number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with 'UniversityName' and 'AdmissionProbability',\n",
        "                          sorted by probability, limited to top_n.\n",
        "                          Returns empty DataFrame on error or no candidates.\n",
        "    \"\"\"\n",
        "    if not candidate_universities:\n",
        "        print(\"Warning: No candidate universities provided.\")\n",
        "        return pd.DataFrame({'UniversityName': [], 'AdmissionProbability': []})\n",
        "\n",
        "    print(f\"Generating predictions for {len(candidate_universities)} candidate universities...\")\n",
        "\n",
        "    # 1. Create input DataFrame for prediction\n",
        "    # Repeat student profile for each candidate university\n",
        "    student_df_list = []\n",
        "    for university in candidate_universities:\n",
        "        profile = student_profile_dict.copy()\n",
        "        profile['UniversityName'] = university # Add the specific university name\n",
        "        student_df_list.append(profile)\n",
        "\n",
        "    input_df = pd.DataFrame(student_df_list)\n",
        "\n",
        "    # Ensure columns match the order expected by the fitted preprocessor\n",
        "    # Add missing columns as NaN if student_profile_dict was partial (might cause issues if not handled in preprocessor)\n",
        "    try:\n",
        "        input_df = input_df[original_feature_columns]\n",
        "    except KeyError as e:\n",
        "         print(f\"Error: Input student profile is missing expected columns: {e}. Cannot predict.\")\n",
        "         print(f\"Expected columns: {original_feature_columns}\")\n",
        "         print(f\"Provided keys: {student_profile_dict.keys()}\")\n",
        "         return pd.DataFrame({'UniversityName': [], 'AdmissionProbability': []})\n",
        "\n",
        "\n",
        "    # 2. Predict Probabilities using the full pipeline\n",
        "    # The pipeline handles preprocessing internally before prediction\n",
        "    try:\n",
        "        print(\"Predicting probabilities...\")\n",
        "        probabilities = model_pipeline.predict_proba(input_df)\n",
        "        # Probabilities array has shape (n_samples, n_classes), we want class 1 (Admitted)\n",
        "        admission_probabilities = probabilities[:, 1]\n",
        "        print(\"Prediction complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        # Potentially print input_df.info() here for debugging\n",
        "        return pd.DataFrame({'UniversityName': [], 'AdmissionProbability': []})\n",
        "\n",
        "\n",
        "    # 3. Rank Universities\n",
        "    results_df = pd.DataFrame({\n",
        "        'UniversityName': candidate_universities,\n",
        "        'AdmissionProbability': admission_probabilities\n",
        "    })\n",
        "\n",
        "    results_df = results_df.sort_values(by='AdmissionProbability', ascending=False)\n",
        "\n",
        "    # 4. Return Top N\n",
        "    return results_df.head(top_n)\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# ** PRE-REQUISITES **\n",
        "# Ensure these exist from previous steps:\n",
        "# - best_xgb_model: The fitted pipeline from GridSearchCV (search_xgb.best_estimator_)\n",
        "# - X_train: The DataFrame used to train the model (needed for column names/order and candidate unis)\n",
        "\n",
        "if 'best_xgb_model' in locals() and 'X_train' in locals():\n",
        "    print(\"\\n--- Generating Example Recommendations ---\")\n",
        "\n",
        "    # 1. Define a New Student Profile (Use exact column names from X_train)\n",
        "    #    Use values *before* scaling/encoding. Fill all relevant features.\n",
        "    new_student = {\n",
        "        'Overall_GPA_4_Scale': 3.7, # Example GPA\n",
        "        'Total Score (Language Exam)': 105.0, # Example TOEFL score\n",
        "        'Language Exam': 'TOEFL',\n",
        "        'Total Score (Competitive Exam)': 325.0, # Example GRE score\n",
        "        'Competitive Exam': 'GRE',\n",
        "        'Future Desired Degree': 'Masters',\n",
        "        'Desired Field of Study': 'Computer Science', # Match categories used in training\n",
        "        'Desired Specialization': 'Software Engineering' # Match categories used in training\n",
        "        # Add any other features your model expects, e.g. UniversityName will be added in the function\n",
        "    }\n",
        "    print(f\"New student profile: {new_student}\")\n",
        "\n",
        "\n",
        "    # 2. Get Candidate Universities (e.g., all unique universities seen in training)\n",
        "    #    Using X_train['UniversityName'] avoids data leakage from test set\n",
        "    try:\n",
        "        candidate_universities = X_train['UniversityName'].unique().tolist()\n",
        "        print(f\"Number of candidate universities from training data: {len(candidate_universities)}\")\n",
        "    except KeyError:\n",
        "         print(\"Error: 'UniversityName' column not found in X_train. Cannot get candidates.\")\n",
        "         candidate_universities = []\n",
        "\n",
        "\n",
        "    # 3. Get Original Feature Columns Order (from X_train used to fit preprocessor)\n",
        "    original_feature_columns = X_train.columns.tolist()\n",
        "\n",
        "\n",
        "    # 4. Call the Recommendation Function\n",
        "    if candidate_universities:\n",
        "        recommendations_df = recommend_universities_xgb(\n",
        "            student_profile_dict=new_student,\n",
        "            model_pipeline=best_xgb_model,\n",
        "            candidate_universities=candidate_universities,\n",
        "            original_feature_columns=original_feature_columns,\n",
        "            top_n=10 # Request top 10 recommendations\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- Top Recommended Universities ---\")\n",
        "        print(recommendations_df)\n",
        "    else:\n",
        "        print(\"\\nCannot generate recommendations as no candidate universities were found.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not run example: 'best_xgb_model' or 'X_train' not found in environment.\")"
      ],
      "metadata": {
        "id": "IJKN05O8ZenI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}